{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Histology Tissue Classification Project (HTCP)"],"metadata":{"id":"RQxvD55PwFJf"}},{"cell_type":"markdown","source":["\n","\n","(C) [K. Mader](https://www.linkedin.com/in/kevinmader/?originalSubdomain=ch) / [U. Michelucci 2018-2019](https://www.linkedin.com/in/umbertomichelucci/?originalSubdomain=ch)\n","\n","*Teaching Assistant:* [Khaled Mohamad](https://www.linkedin.com/in/khaled-mohamad-45071a24b/).\n","\n","# Overview\n","\n","The dataset serves as a much more interesting MNIST or CIFAR10 problem for biologists by focusing on histology tiles from patients with colorectal cancer. In particular, the data has 8 different classes of tissue (but Cancer/Not Cancer can also be an interesting problem).\n","\n","The dataset has been adapted for the course by K. Mader (kevin.mader@gmail.com), and is available on kaggle: https://goo.gl/26zj41\n","\n","# Challenge\n","\n","* Classify tiles correctly into one of the eight classes\n","* Which classes are most frequently confused?\n","* What features can be used (like texture, see scikit-image) to improve classification?\n","* How can these models be applied to the much larger 5000x5000 models? How can this be done efficiently\n","\n","# Acknowledgements\n","\n","\n","The dataset has been copied from Zenodo: https://zenodo.org/record/53169#.W6HwwP4zbOQ\n","\n","made by: Kather, Jakob Nikolas; Zöllner, Frank Gerrit; Bianconi, Francesco; Melchers, Susanne M; Schad, Lothar R; Gaiser, Timo; Marx, Alexander; Weis, Cleo-Aron\n","\n","The copy here is to make it more accessible to Kaggle users and allow kernels providing basic analysis of the data\n","\n","Content This data set represents a collection of textures in histological images of human colorectal cancer. It contains two files:\n","\n",">     Kather_texture_2016_image_tiles_5000.zip\": a zipped folder containing 5000 \n",">     histological images of 150 * 150 px each (74 * 74 µm). Each image belongs \n",">     to exactly one of eight tissue categories (specified by the folder name). \n","\n",">     Kather_texture_2016_larger_images_10.zip\": a zipped folder containing 10 \n",">     larger histological images of 5000 x 5000 px each. These images contain \n",">     more than one tissue type. Image format\n","\n","\n","All images are RGB, 0.495 µm per pixel, digitized with an Aperio ScanScope \n","(Aperio/Leica biosystems), magnification 20x. Histological samples are fully \n","anonymized images of formalin-fixed paraffin-embedded human colorectal \n","adenocarcinomas (primary tumors) from our pathology archive (Institute of Pathology, \n","University Medical Center Mannheim, Heidelberg University, Mannheim, Germany).\n","\n","Additionally the files has been prepared to resemble the MNIST dataset, meaning that you will also find the following files\n","\n","- HTCP_8_8_L - \n","- HTCP_8_8_RGB -\n","- HTCP_28_28_L -\n","- HTCP_28_28_RGB - \n","- HTCP_64_64_L\n","\n","# Ethics statement\n","All experiments were approved by the institutional ethics board (medical ethics board II, University Medical Center Mannheim, Heidelberg University, Germany; approval 2015-868R-MA). The institutional ethics board waived the need for informed consent for this retrospective analysis of anonymized samples. All experiments were carried out in accordance with the approved guidelines and with the Declaration of Helsinki.\n","\n","# More information / data usage\n","For more information, please refer to the following article. Please cite this article when using the data set.\n","\n","Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press)\n","\n","# Contact\n","For questions, please contact: Dr. Jakob Nikolas Kather http://orcid.org/0000-0002-3730-5348 ResearcherID: D-4279-2015\n"],"metadata":{"id":"NStyZnPF2dwO"}},{"cell_type":"markdown","source":["# Download the data"],"metadata":{"id":"uBrmZ0hC298g"}},{"cell_type":"markdown","source":["The dataset is composed of two datasets:\n","\n","- The small images that will be used to test the classification models\n","- The big microscope images (5000x5000)\n","\n","The first dataset is quite small and can be found in the same github repository where you find this file. The second are much bigger (250 Mb and 700 Mb) and cannot be uploaded on github, so you can get them on kaggle: https://goo.gl/hkRSke"],"metadata":{"id":"-ug-JIdU3HYa"}},{"cell_type":"markdown","source":["# Ideas for the project\n","\n","The project can be tackled in several ways and at several levels. Here are some ideas for you to tackle at different difficulty levels.\n","\n","A few general hints:\n","\n","- Accuracy is a nice metric, but in this case the confusion matrix is more useful. Check which metric is the most ideal for this problem (you could use others)\n","- If detecting TUMOR proces too hard, try to detect other tissue types. For example ADIPOSE. Some are much easier to detect than others. \n","- __REMEMBER__: detecting __ONE__ type of tissue does not necessarly mean being able to detec __ALL__ type of tissues well ;-)\n","- __REMEMBER__: getting a high accuracy is __NOT__ the goal of the project. The goal is to put you in a real-life situation where you have to be creative to solve a relevant problem. Is not easy and there are not easy ways of solving it.\n","\n","__OVER ALL REMEMBER: HAVE FUN!__\n","\n","\n","## Advance\n","\n","If you have studied the color distribution in the Easy exercise part, try to build your first pipeline of models: first a manually build classifier (based on distributions) to eliminate easy to detect classes and then a network for the most complex cases.\n","\n","# Fun!\n","After having trained a model to classify images, build a \"probability map\" over the 5000 pixels x 5000 pixels images to detect specific tissue types. A way in which you could do it is to make sliding window on the big image, and let the model give the probability of each slide and then plot an overlay on the big image with (for example) the color intensity proprortional to the probability your model is predicting.\n","WARNING: this will take some time, but it is really be fun!\n","\n"],"metadata":{"id":"UvhhfhKN3qh8"}},{"cell_type":"markdown","source":["\n","##__STARTING WITH__:\n","## 2. Advance!"],"metadata":{"id":"GsUGv_iA7ZRP"}},{"cell_type":"markdown","source":["# Helper Functions (Python)\n","##### __Your__ first task, is to __uncomment ( Ctrl + / )__ ploting images function and run your cell!"],"metadata":{"id":"DmA9x2MC5rad"}},{"cell_type":"markdown","source":["#### __#__ A function for plotting images"],"metadata":{"id":"KreNzpjQyESe"}},{"cell_type":"code","source":["\n","\n","# def plot_image(some_image):\n","    \n","#     some_digit_image = some_image.values.reshape(28,28)\n","\n","#     plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n","#     plt.axis(\"off\")\n","#     plt.show()\n","    "],"metadata":{"id":"zy6OyIyl419w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A function to get Label names(are eight labels)\n","##### __uncomment__ the cell below and run it"],"metadata":{"id":"L3x-8ExtynuC"}},{"cell_type":"code","source":["\n","\n","# def get_label_name(idx):\n","    \n","#     if (idx == 1):\n","#         return '(1) TUMOR'\n","#     elif (idx == 2):\n","#         return '(2) STROMA'\n","#     elif (idx == 3):\n","#         return '(3) COMPLEX'\n","#     elif (idx == 4):\n","#         return '(4) LYMPHO'\n","#     elif (idx == 5):\n","#         return '(5) DEBRIS'\n","#     elif (idx == 6):\n","#         return '(6) MUCOSA'\n","#     elif (idx == 7):\n","#         return '(7) ADIPOSE'\n","#     elif (idx == 8):\n","#         return '(8) EMPTY'"],"metadata":{"id":"2rJr2R3J_Bnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load & Importing Libraries"],"metadata":{"id":"Xm5mqcLFCbY-"}},{"cell_type":"code","source":["# Load python libraries\n","\n","%matplotlib inline\n","from glob import glob\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import numpy as np\n","import pandas as pd\n","from random import randint\n","\n","# Read images from files and plot\n","from skimage.io import imread \n","import seaborn as sns\n","\n","# Tensorflow & Keras is imported for building and training models\n","import tensorflow as tf\n","\n","# Keras \n","from tensorflow.keras.models import Sequential # for building the  layers\n","from tensorflow.keras.optimizers import SGD    # Optimizer \n","from tensorflow.keras.layers import Dense      # Connected network\n","\n","from tensorflow.keras import layers\n","import tensorflow.keras as keras\n","from sklearn.metrics import confusion_matrix, accuracy_score   # Measure performance of your classifier and accuracy\n","import time"],"metadata":{"id":"6JiMvE9HCMP8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Checking your TensorFlow Version\n","\n","##### __Uncomment__ the cell below and run it"],"metadata":{"id":"YyhqRLixVZx_"}},{"cell_type":"code","source":["\n","# ver_1= '2.10.0'\n","\n","# def tf_version(tf):\n","#   tf_v = tf.__version__\n","#   if tf_v >= ver_1:\n","#     print(f\" Your veriosn of TensorFlow is a:{tf_v}  satisfied!\")\n","#   else:\n","#     print(\"Your new version of TensroFlow updating ....\")\n","#     !pip3 install --upgrade tensorflow\n","\n","# tf_version(tf)\n"],"metadata":{"id":"jLr6jKBmzvwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load the Data\n","#### __Replace__ your current directory path with '/content/data\n","#### __Uncomment__ the last two lines of code below USING ( Ctrl + / ) and run your cell"],"metadata":{"id":"NbnSJU80HNSS"}},{"cell_type":"code","source":["# Know Image Dimensions\n","def know_image_dim(in_shape):\n","\n","    side_len = int(np.sqrt(in_shape))\n","    abs_value = np.abs(in_shape-side_len*side_len)<2\n","    negative_value = side_len = int(np.sqrt(in_shape/3))\n","\n","    if abs_value:\n","        return (int(side_len), int(side_len))\n","    else:\n","        negative_value\n","        return (side_len, side_len, 3)\n","        \n","# csv_dir = os.path.join('.', '/content/data')\n","# print(f\"My current working directory is: {csv_dir} \")"],"metadata":{"id":"tBW-NC0O0qDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print your Vector size for evey dataset\n","#### __Uncomment__ code below and run your cell"],"metadata":{"id":"EoqUpcOl03Jg"}},{"cell_type":"code","source":["# Return all file names from current directory and sort in ascending orders(shaps)\n","all_files = sorted(glob(os.path.join(csv_dir, 'HTCP*.csv')), \n","                   key=lambda x: os.stat(x).st_size)\n","\n","\n","# all_df_dict = {os.path.splitext(os.path.basename(x))[0]: pd.read_csv(x) for x in all_files}\n","# print(\"VECTOR SIZE FOR EVERY DATASET:\\n\")\n","# for c_key in all_df_dict.keys():\n","#     print(c_key, 'vector length:',  \n","#           all_df_dict[c_key].shape[1], '->', \n","#             know_image_dim(all_df_dict[c_key].shape[1]))"],"metadata":{"id":"rZc9I6wa07Od"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print the folder and data paths\n","#### __Uncomment__ all_files and run"],"metadata":{"id":"4N8dHJz-1CkO"}},{"cell_type":"code","source":["# all_files"],"metadata":{"id":"Tt0y7MPP1H7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read csv file from list(all_files)\n","data = pd.read_csv(all_files[2])"],"metadata":{"id":"t5pMU4SYHe9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's create an array with labels (not yet one-encoded) and one for the images."],"metadata":{"id":"xmGxOrgpIA_h"}},{"cell_type":"code","source":["# Get the labels from the data\n","labels = data['label']\n","data = data.drop(['label'], axis = 1)\n"],"metadata":{"id":"PoO_0hLaza48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline models with 28x28 gray images\n","\n","### Import python libraries\n","##### __Uncomment__ code below and run your cell"],"metadata":{"id":"7givs_A1a34k"}},{"cell_type":"code","source":["# from sklearn.model_selection import train_test_split\n","# sample_id_count = list(all_df_dict.values())[0].shape[0]\n","# train_ids, test_ids = train_test_split(range(sample_id_count), \n","#                                        test_size=0.25, \n","#                                        random_state=2018)"],"metadata":{"id":"9olNbtq1a44l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Measure Accurcy and loss error\n"],"metadata":{"id":"Kts8dKrH2d4y"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","def evaluate_models(in_model_maker):\n","    fig, m_axs = plt.subplots(1, 5, figsize = (25, 5))\n","    for c_ax, c_key in zip(m_axs, all_df_dict.keys()):\n","        # c_key is for example HTCP_8_8_L (the file/type name)\n","        c_df = all_df_dict[c_key].copy()\n","        c_label = c_df.pop('label') # return column and drop from dataframe\n","        c_model = in_model_maker() # function of the model\n","        c_model.fit(c_df.iloc[train_ids, :], c_label.iloc[train_ids]) # fit of the model\n","        c_pred = c_model.predict(c_df.iloc[test_ids, :]) # prediction\n","        sns.heatmap(confusion_matrix(c_label.iloc[test_ids], c_pred), \n","                    annot=True, cbar=False, fmt='d', ax=c_ax)\n","        c_ax.set_title(f'Accuracy: {accuracy_score(c_label[test_ids],c_pred)*100:2.2f}%\\n{c_key}')"],"metadata":{"id":"ll0wA1PLa7h2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example of a network with 4 layers, each with 10 neurons"],"metadata":{"id":"VYZqpFCPZR5t"}},{"cell_type":"code","source":["class NBatchLogger(keras.callbacks.Callback):\n","    \"\"\"\n","    A Logger that log average performance per `display` steps.\n","    \"\"\"\n","    def __init__(self, display):\n","        self.step = 0\n","        self.display = display\n","        self.metric_cache = {}\n","\n","    def on_batch_end(self, batch, logs=None):\n","        self.step += 1\n","        for k in self.model.metrics:\n","            if k.name not in self.metric_cache.keys():\n","                self.metric_cache[k.name] = 0.0\n","            self.metric_cache[k.name] += logs.get(k.name)\n","        if self.step % self.display == 0:\n","            metrics_log = ''\n","            for (k, v) in self.metric_cache.items():\n","                val = v / self.display\n","                if abs(val) > 1e-3:\n","                    metrics_log += ' - %s: %.4f' % (k, val)\n","                else:\n","                    metrics_log += ' - %s: %.4e' % (k, val)\n","            print('step: {}/{} ... {}'.format(self.step,\n","                                          self.params['steps'],\n","                                          metrics_log))\n","            self.metric_cache.clear()"],"metadata":{"id":"YHXXq9NnZQVE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make a copy of Dataset and Labels.\n","###### __Uncomment__ code below and run your cell"],"metadata":{"id":"OGl6DBt228xH"}},{"cell_type":"code","source":["\n","# c_df = all_df_dict['HTCP_28_28_L'].copy()\n","# c_label = c_df.pop('label')\n"],"metadata":{"id":"TwU0ePL4akh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split dataset to train and test \n","###### __Uncomment__ code below and run your cell"],"metadata":{"id":"ggBPT8Dq3QxA"}},{"cell_type":"code","source":["\n","# X_train = c_df.iloc[train_ids, :]\n","# y_train = c_label.iloc[train_ids]-1"],"metadata":{"id":"Tj3v0O-5a6bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X_test = c_df.iloc[test_ids, :]\n","# ytest = c_label.iloc[test_ids]-1"],"metadata":{"id":"wl6O0aiLbIkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print shape of your data"],"metadata":{"id":"0Kld0QQV3mDn"}},{"cell_type":"code","source":["#X_train.shape"],"metadata":{"id":"7lkU7v7J31ZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert class vectors to binary class matrices One Hot Encoding\n","num_of_classes = 8\n","y_train = keras.utils.to_categorical(y_train, num_of_classes)\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, \n","                                                  test_size = 0.1, random_state=42)\n","\n","X_train = X_train / 255.0 / 1.0\n","X_val = X_val / 255.0 / 1.0"],"metadata":{"id":"5En0SCTDd2Ii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building Neural Network Phases\n","#### __Your task__ change activation function to relu:    (activation='relu')\n","######1. __Uncomment__ the 3 lines of the code below first!\n","######2. __Change__ your optimizers to Adam : optimizer=tf.optimizers.Adam(0.01)"],"metadata":{"id":"UQCh6e2j4ClI"}},{"cell_type":"code","source":["n = 15\n","model = tf.keras.Sequential()\n","model.add(layers.Dense(n, input_dim=X_train.shape[1], activation='relu'))\n","# model.add(layers.Dense(n, activation='relu'))\n","model.add(layers.Dropout(0.40))\n","# model.add(layers.Dense(n, activation='relu'))\n","model.add(layers.Dropout(0.40))\n","# model.add(layers.Dense(n, activation='relu'))\n","model.add(layers.Dropout(0.40))\n","model.add(layers.Dense(num_of_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              # optimizer=tf.optimizers,\n","              metrics=['accuracy'])\n","    \n","model.summary()"],"metadata":{"id":"p9ZjxPl54QT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train dataset with 1000 iterations\n","##### __Uncomment__ the code below and run"],"metadata":{"id":"EkB-H5X64m5Z"}},{"cell_type":"code","source":["out_batch = NBatchLogger(display=1000)\n","model.fit(X_train, Y_train, epochs=1000, batch_size=250,verbose = 0,\n","             callbacks=[out_batch])\n"],"metadata":{"id":"ERK8yhCq4pV-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Measure your final loss and accuracy model\n","##### __Uncomment__ the code below and run the cell"],"metadata":{"id":"kd1vZtUG4t8j"}},{"cell_type":"code","source":["# final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=1)\n","# pred = np.argmax(model.predict(X_val), axis=-1)\n","# pred_train = np.argmax(model.predict(X_train), axis=-1)\n","# print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"],"metadata":{"id":"wEe8p3pi4wKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize your Confusion Matrix for trained dataset\n","##### __Uncomment__ the code below and run the cell"],"metadata":{"id":"WdKjIHbre_Y_"}},{"cell_type":"code","source":["# fig = plt.figure(figsize=(7,7))\n","# ax = fig.add_subplot(1, 1, 1)\n","# sns.heatmap(confusion_matrix(np.argmax(Y_train, axis = 1), pred_train), \n","#                     annot=True, cbar=False, fmt='d', ax=ax)"],"metadata":{"id":"8RzqPCC748oG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Confusion Matrix for dev\n","###### __Uncomment__ the code below and run the cell"],"metadata":{"id":"zxRvVBXkfI5v"}},{"cell_type":"code","source":["# fig = plt.figure(figsize=(7,7))\n","# ax = fig.add_subplot(1, 1, 1)\n","# sns.heatmap(confusion_matrix(np.argmax(Y_val, axis = 1), pred), \n","#                     annot=True, cbar=False, fmt='d', ax=ax)"],"metadata":{"id":"j9uxJPNN5b2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building CNN Architecture"],"metadata":{"id":"_ctXR1k05eIm"}},{"cell_type":"markdown","source":["##### __CNN__ using tensorflow"],"metadata":{"id":"nvSSIzQd3izP"}},{"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"metadata":{"id":"qUJv9c-l7y66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["######1. Creaing a Placeholder variable for the input images 28*28\n","######2. Reshaping image size [num_images, img_height, img_width, num_channels]\n","\n","######3. Placeholder variable for the true labels associated with the images\n","\n","##### __Uncomment__ the code below and run the cell"],"metadata":{"id":"ord7LC0160nv"}},{"cell_type":"code","source":["\n","# x = tf.placeholder(tf.float32, shape=[None, 28*28], name='X')\n","# x_image = tf.reshape(x, [-1, 28, 28, 1])\n","\n","# y_true = tf.placeholder(tf.float32, shape=[None, 8], name='y_true')\n","# y_true_cls = tf.argmax(y_true, axis=1)"],"metadata":{"id":"Na-lJdC23k9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A function taking data input, kernel(matrix size [1, 2, 2, 1])"],"metadata":{"id":"0HxN8yKC8GEl"}},{"cell_type":"code","source":["def new_conv_layer(input, num_input_channels, filter_size, num_filters):\n","    \n","        # Shape of the filter-weights for the convolution\n","    shape = [filter_size, filter_size, num_input_channels, num_filters]\n","\n","        # Create new weights (filters) with the given shape\n","    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n","\n","        # Create new biases, one for each filter\n","    biases = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n","\n","        # TensorFlow operation for convolution\n","    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n","\n","        # Add the biases to the results of the convolution.\n","    layer += biases\n","        \n","    return layer, weights"],"metadata":{"id":"8fe2L27x_XJV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Input to pooling layer\n","##### __Uncomment__ the cell below and run it"],"metadata":{"id":"xs2IFTPf9di0"}},{"cell_type":"code","source":["# def new_pool_layer(input):\n","    \n","#     layer = tf.nn.max_pool(value=input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","        \n","#     return layer"],"metadata":{"id":"FxaWabRX_cY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Applying relu activation function on the input"],"metadata":{"id":"2SZUHdHL-GGh"}},{"cell_type":"code","source":["def new_relu_layer(input):\n","    \n","    #with tf.variable_scope(name) as scope:\n","    # TensorFlow operation for convolution\n","    layer = tf.nn.relu(input)\n","        \n","    return layer"],"metadata":{"id":"N5NU0xhp_fS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def new_fc_layer(input, num_inputs, num_outputs):\n","         # Create new weights and biases.\n","    weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05))\n","    biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n","        \n","        # Multiply the input and weights, and then add the bias-values.\n","    layer = tf.matmul(input, weights) + biases\n","        \n","    return layer"],"metadata":{"id":"DcyDvyXV_js6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building Convolutional and Pooling Layers\n","###### __Your__ task now, for Convolutional Layer 1, change number of input channels = 1 and filter size = 5 and number of filters= 32 then run the cell"],"metadata":{"id":"zskivSS2-hFH"}},{"cell_type":"code","source":["# Convolutional Layer 1\n","# Make changes here!\n","layer_conv1, weights_conv1 = new_conv_layer(input=x_image, num_input_channels= , filter_size=, num_filters=) # 6\n","\n","# Pooling Layer 1\n","layer_pool1 = new_pool_layer(layer_conv1)\n","\n","# RelU layer 1\n","layer_relu1 = new_relu_layer(layer_pool1)\n","\n","# Convolutional Layer 2\n","layer_conv2, weights_conv2 = new_conv_layer(input=layer_relu1, num_input_channels=32, filter_size=5, num_filters=32)\n","\n","# Pooling Layer 2\n","layer_pool2 = new_pool_layer(layer_conv2)\n","\n","# RelU layer 2\n","layer_relu2 = new_relu_layer(layer_pool2)\n","\n","# Flatten Layer\n","num_features = layer_relu2.get_shape()[1:4].num_elements()\n","layer_flat = tf.reshape(layer_relu2, [-1, num_features])\n","\n","# Fully-Connected Layer 1\n","layer_fc1 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=128)\n","\n","# RelU layer 3\n","layer_relu3 = new_relu_layer(layer_fc1)\n","\n","# Fully-Connected Layer 2\n","layer_fc2 = new_fc_layer(input=layer_relu3, num_inputs=128, num_outputs=8)"],"metadata":{"id":"IM4s0d-O_kw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # Use Softmax function to normalize the output\n","###### __Uncomment__ the cell below and run it"],"metadata":{"id":"k9z2KeIXAIt6"}},{"cell_type":"code","source":["\n","# y_pred = tf.nn.softmax(layer_fc2)\n","# y_pred_cls = tf.argmax(y_pred, axis=1)"],"metadata":{"id":"BSFpwVd4_o1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Use Cross entropy cost function\n","###### __Uncomment__ the cell below and run it"],"metadata":{"id":"_FVTH5gQAdpz"}},{"cell_type":"code","source":["\n","# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)\n","# cost = tf.reduce_mean(cross_entropy)"],"metadata":{"id":"iRcdXCcAAmME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Use Adam Optimizer\n","#### __Your__ task is to assign a learning rate to : 1e-3"],"metadata":{"id":"7CIVocGIArdh"}},{"cell_type":"code","source":["\n","optimizer = tf.train.AdamOptimizer(learning_rate=).minimize(cost)"],"metadata":{"id":"XXmRqK4b_vXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy\n","correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"metadata":{"id":"t9JFuI32_0l3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Number of training iterations\n","\n","##### __Your__ task, give number of epochs = 100 or more if you like! and batch size = 30, try more if you like\n"],"metadata":{"id":"X1qGEvy7CFdD"}},{"cell_type":"code","source":["num_epochs = \n","batch_size = \n"],"metadata":{"id":"PpChgA0Z_5VE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split data into train and validation set\n","##### __Uncomment__ the celll below and run it"],"metadata":{"id":"3lzHJldUBkDi"}},{"cell_type":"code","source":["# X_train = np.array(X_train)\n","# Y_train = np.array(Y_train)\n","# X_val = np.array(X_val)\n","# Y_val = np.array(Y_val)"],"metadata":{"id":"N03VnAxW_8FV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training the model for 100 epochs.\n","###### __Uncomment__ last two prints lins in very end of this cell to print training and validation Accuracy"],"metadata":{"id":"FmugkFfzC-lj"}},{"cell_type":"code","source":["print (\"Training the model for\", num_epochs, \"epochs.\")\n","with tf.Session() as sess:\n","    # Initialize all variables\n","    sess.run(tf.global_variables_initializer())\n","    \n","    # Add the model graph to TensorBoard\n","    #writer.add_graph(sess.graph)\n","    \n","    # Loop over number of epochs\n","    for epoch in range(num_epochs):\n","        \n","        start_time = time.time()\n","        train_accuracy = 0\n","        \n","        #for batch in range(0, int(len(labels_)/batch_size)):\n","        # X_train, Y_train\n","        # X_val, Y_val,\n","            \n","        for i in range(0, X_train.shape[0], batch_size):\n","            #if (i%1000 == 0):\n","            #    print('-->',i)\n","            x_batch = X_train[i:i + batch_size,:]\n","            y_true_batch = Y_train[i:i + batch_size,:]\n","            \n","            # Put the batch into a dict with the proper names for placeholder variables\n","            feed_dict_train = {x: x_batch, y_true: y_true_batch}\n","            \n","            # Run the optimizer using this batch of training data.\n","            sess.run(optimizer, feed_dict=feed_dict_train)\n","            \n","        train_accuracy = sess.run(accuracy, feed_dict={x:X_train, y_true:Y_train})\n","        \n","        # Generate summary and validate the model on the entire validation set\n","        vali_accuracy = sess.run(accuracy, feed_dict={x:X_val, y_true:Y_val})\n","        #writer1.add_summary(summ, epoch)\n","        \n","\n","        end_time = time.time()\n","        \n","        if (epoch % 5 == 0):\n","            print(\"Epoch \"+str(epoch+1)+\" completed : Time usage \"+str(int(end_time-start_time))+\" seconds\")\n","            print(\"\\tAccuracy:\")\n","            print (\"\\t- Training Accuracy:\\t{}\".format(train_accuracy))\n","            print (\"\\t- Validation Accuracy:\\t{}\".format(vali_accuracy))\n","            \n","    pred = sess.run(y_pred_cls, feed_dict={x:X_train, y_true:Y_train})\n","            \n","print(\"\\n\\nFinal Accuracy:\")\n","\n","\n","# print (\"- Training Accuracy:\\t{}\".format(train_accuracy))\n","# print (\"- Validation Accuracy:\\t{}\".format(vali_accuracy))"],"metadata":{"id":"ujZHPgdPEDJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print confusion_matrix\n","##### __uncomment__ the cell below and run "],"metadata":{"id":"geO4b-YyEGOA"}},{"cell_type":"code","source":["# fig = plt.figure(figsize=(7,7))\n","# ax = fig.add_subplot(1, 1, 1)\n","# sns.heatmap(confusion_matrix(np.argmax(Y_train, axis = 1), pred), \n","#                     annot=True, cbar=False, fmt='d', ax=ax)"],"metadata":{"id":"IcV4T65SEbDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VRgJYTKAEoRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __Now__ Your last challenge!\n","\n","*By taking a looking to your __Training__ data accuracy should be around __98__ and __Validation__ accuracy should be around __63__: This mean you model do very well in training data but not well in validation data. It's clear that the model suffer from overfitting.*\n","\n","''' __Your__ challenge will be to add handle overfitting!!, Follow the steps below '''\n","\n","#### __Steps for reducing overfitting:__\n","1. Add more data(Use data augmentation)\n","2. Reduce the network’s capacity\n","3. Add regularization( Dropout layers)\n","4. Transfer Learning\n","\n","### Here the link: [steps](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e) need to be done to reduce the overfitting! \n","\n","###For link [Transfer learning](https://www.youtube.com/watch?v=zBOavqh3kWU&ab_channel=KrishNaik) you will find here!\n","\n","\n","__Surprise us with your result!!!__\n"],"metadata":{"id":"1ZAgIEpuEq2_"}}]}