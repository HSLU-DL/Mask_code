{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Histology Tissue Classification Project (HTCP)"],"metadata":{"id":"RQxvD55PwFJf"}},{"cell_type":"markdown","source":["\n","\n","\n","(C) [K. Mader](https://www.linkedin.com/in/kevinmader/?originalSubdomain=ch) / [U. Michelucci 2018-2019](https://www.linkedin.com/in/umbertomichelucci/?originalSubdomain=ch)\n","\n","*Teaching Assistant:* [Khaled Mohamad](https://www.linkedin.com/in/khaled-mohamad-45071a24b/).\n","\n","\n","\n","\n","# Overview\n","\n","The dataset serves as a much more interesting MNIST or CIFAR10 problem for biologists by focusing on histology tiles from patients with colorectal cancer. In particular, the data has 8 different classes of tissue (but Cancer/Not Cancer can also be an interesting problem).\n","\n","The dataset has been adapted for the course by K. Mader (kevin.mader@gmail.com), and is available on kaggle: https://goo.gl/26zj41\n","\n","# Challenge\n","\n","* Classify tiles correctly into one of the eight classes\n","* Which classes are most frequently confused?\n","* What features can be used (like texture, see scikit-image) to improve classification?\n","* How can these models be applied to the much larger 5000x5000 models? How can this be done efficiently\n","\n","# Acknowledgements\n","\n","\n","The dataset has been copied from Zenodo: https://zenodo.org/record/53169#.W6HwwP4zbOQ\n","\n","made by: Kather, Jakob Nikolas; Zöllner, Frank Gerrit; Bianconi, Francesco; Melchers, Susanne M; Schad, Lothar R; Gaiser, Timo; Marx, Alexander; Weis, Cleo-Aron\n","\n","The copy here is to make it more accessible to Kaggle users and allow kernels providing basic analysis of the data\n","\n","Content This data set represents a collection of textures in histological images of human colorectal cancer. It contains two files:\n","\n",">     Kather_texture_2016_image_tiles_5000.zip\": a zipped folder containing 5000 \n",">     histological images of 150 * 150 px each (74 * 74 µm). Each image belongs \n",">     to exactly one of eight tissue categories (specified by the folder name). \n","\n",">     Kather_texture_2016_larger_images_10.zip\": a zipped folder containing 10 \n",">     larger histological images of 5000 x 5000 px each. These images contain \n",">     more than one tissue type. Image format\n","\n","\n","All images are RGB, 0.495 µm per pixel, digitized with an Aperio ScanScope \n","(Aperio/Leica biosystems), magnification 20x. Histological samples are fully \n","anonymized images of formalin-fixed paraffin-embedded human colorectal \n","adenocarcinomas (primary tumors) from our pathology archive (Institute of Pathology, \n","University Medical Center Mannheim, Heidelberg University, Mannheim, Germany).\n","\n","Additionally the files has been prepared to resemble the MNIST dataset, meaning that you will also find the following files\n","\n","- HTCP_8_8_L - \n","- HTCP_8_8_RGB -\n","- HTCP_28_28_L -\n","- HTCP_28_28_RGB - \n","- HTCP_64_64_L\n","\n","# Ethics statement\n","All experiments were approved by the institutional ethics board (medical ethics board II, University Medical Center Mannheim, Heidelberg University, Germany; approval 2015-868R-MA). The institutional ethics board waived the need for informed consent for this retrospective analysis of anonymized samples. All experiments were carried out in accordance with the approved guidelines and with the Declaration of Helsinki.\n","\n","# More information / data usage\n","For more information, please refer to the following article. Please cite this article when using the data set.\n","\n","Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press)\n","\n","# Contact\n","For questions, please contact: Dr. Jakob Nikolas Kather http://orcid.org/0000-0002-3730-5348 ResearcherID: D-4279-2015\n"],"metadata":{"id":"NStyZnPF2dwO"}},{"cell_type":"markdown","source":["# Download the data"],"metadata":{"id":"uBrmZ0hC298g"}},{"cell_type":"markdown","source":["The dataset is composed of two datasets:\n","\n","- The small images that will be used to test the classification models\n","- The big microscope images (5000x5000)\n","\n","The first dataset is quite small and can be found in the same github repository where you find this file. The second are much bigger (250 Mb and 700 Mb) and cannot be uploaded on github, so you can get them on kaggle: https://goo.gl/hkRSke"],"metadata":{"id":"-ug-JIdU3HYa"}},{"cell_type":"markdown","source":["# Ideas for the project\n","\n","The project can be tackled in several ways and at several levels. Here are some ideas for you to tackle at different difficulty levels.\n","\n","A few general hints:\n","\n","- Accuracy is a nice metric, but in this case the confusion matrix is more useful. Check which metric is the most ideal for this problem (you could use others)\n","- If detecting TUMOR proces too hard, try to detect other tissue types. For example ADIPOSE. Some are much easier to detect than others. \n","- __REMEMBER__: detecting __ONE__ type of tissue does not necessarly mean being able to detec __ALL__ type of tissues well ;-)\n","- __REMEMBER__: getting a high accuracy is __NOT__ the goal of the project. The goal is to put you in a real-life situation where you have to be creative to solve a relevant problem. Is not easy and there are not easy ways of solving it.\n","\n","__OVER ALL REMEMBER: HAVE FUN!__\n","\n","## Easy\n","\n","- Use the gray level 28x28 images and consider only two classes: TUMOR and (for example) ADIPOSE. Build with a classifier with a neural network of your choice (probably one neuron would be enough). Similary to what we have done in the class with the digits 1 and 2 from the MNIST dataset.\n","- Try to build a classifier using the distribution of gray levels in each class. You should take all the images of a specific class and plot the gray level distributions to see if you can use it.\n","- Try to build a classifier using the distribution of each color channel levels in each class. You should take all the images of a specific class and plot the color channel level distributions to see if you can use it.\n","- Build a model that gives as output not only the predicted classes, but all 8 with the probability each has sorted from the highest probability to the lowest.\n","\n"],"metadata":{"id":"UvhhfhKN3qh8"}},{"cell_type":"markdown","source":["\n","##__STARTING WITH__:\n","## 1. Easy!"],"metadata":{"id":"GsUGv_iA7ZRP"}},{"cell_type":"markdown","source":["# Helper Functions (Python)"],"metadata":{"id":"DmA9x2MC5rad"}},{"cell_type":"code","source":["# A function for plotting images\n","\n","def plot_image(some_image):\n","    \n","    some_digit_image = some_image.values.reshape(28,28)\n","\n","    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n","    plt.axis(\"off\")\n","    plt.show()\n","    "],"metadata":{"id":"zy6OyIyl419w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to get Label names(are eight)\n","\n","def get_label_name(idx):\n","    \n","    if (idx == 1):\n","        return '(1) TUMOR'\n","    elif (idx == 2):\n","        return '(2) STROMA'\n","    elif (idx == 3):\n","        return '(3) COMPLEX'\n","    elif (idx == 4):\n","        return '(4) LYMPHO'\n","    elif (idx == 5):\n","        return '(5) DEBRIS'\n","    elif (idx == 6):\n","        return '(6) MUCOSA'\n","    elif (idx == 7):\n","        return '(7) ADIPOSE'\n","    elif (idx == 8):\n","        return '(8) EMPTY'"],"metadata":{"id":"2rJr2R3J_Bnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load & Importing Libraries"],"metadata":{"id":"Xm5mqcLFCbY-"}},{"cell_type":"code","source":["# Load python libraries\n","\n","%matplotlib inline\n","from glob import glob\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import numpy as np\n","import pandas as pd\n","from random import randint\n","\n","# Read images from files and plot\n","from skimage.io import imread \n","import seaborn as sns\n","\n","# Tensorflow & Keras is imported for building and training models\n","import tensorflow as tf\n","\n","# Keras \n","from tensorflow.keras.models import Sequential # for building the  layers\n","from tensorflow.keras.optimizers import SGD    # Optimizer \n","from tensorflow.keras.layers import Dense      # Connected network\n","\n","from tensorflow.keras import layers\n","import tensorflow.keras as keras\n","from sklearn.metrics import confusion_matrix, accuracy_score   # Measure performance of your classifier and accuracy\n","import time"],"metadata":{"id":"6JiMvE9HCMP8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Checking your TensorFlow Version "],"metadata":{"id":"ycrwWqdHqFQA"}},{"cell_type":"markdown","source":["#### Your very first task to check your TensorFlow version!\n","#### To uncomment the code below use (Ctrl + / )"],"metadata":{"id":"KbBu8HGCGuQS"}},{"cell_type":"code","source":["\n","# ver_1= '2.10.0'\n","\n","# def tf_version(tf):\n","#   tf_v = tf.__version__\n","#   if tf_v >= ver_1:\n","#     print(f\" Your veriosn of TensorFlow is a:{tf_v}  satisfied!\")\n","#   else:\n","#     print(\"Your new version of TensroFlow updating ....\")\n","#     !pip3 install --upgrade tensorflow\n","\n","# tf_version(tf)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ppzv9FbaND29","outputId":"7d8be085-ac00-41cb-db18-8c98ada102e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Your veriosn of TensorFlow is a:2.10.0  satisfied!\n"]}]},{"cell_type":"markdown","source":["# Load the Data\n","### Your second task to load the Kather dataset\n","#### Steps:\n","##### 1. Download the \"Kather_texture_2016_larger_images_10\" to your pc\n","##### 2. Import the dataset to google colab as a zip file\n","##### 3. Uncomment the code below using (Ctrl + / ) \n","##### 4. Unzip the file by run the code below! \n","##### 5. Refresh your folder\n","\n"],"metadata":{"id":"NbnSJU80HNSS"}},{"cell_type":"code","source":["\n","# from zipfile import ZipFile\n","# file_name = \"/content/Kather_texture_2016_image_tiles_5000\"\n","# with ZipFile(file_name, 'r') as zip:\n","#   zip.extractall()\n","#   print(\"Done!\")"],"metadata":{"id":"yZranR8EzZbv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find your working directory path\n","##### 1. Uncomment the code below using (Ctrl + / )\n","##### 2. Right-click on the dataset you uploaded it and copy the path and replace it with '/content/sample_data/data'\n","##### 3. Run the code"],"metadata":{"id":"BqgJm5rzLh-A"}},{"cell_type":"code","source":["\n","# def know_image_dim(in_shape):\n","\n","#     side_len = int(np.sqrt(in_shape))\n","#     abs_value = np.abs(in_shape-side_len*side_len)<2\n","#     negative_value = side_len = int(np.sqrt(in_shape/3))\n","\n","#     if abs_value:\n","#         return (int(side_len), int(side_len))\n","#     else:\n","#         negative_value\n","#         return (side_len, side_len, 3)\n","        \n","# csv_dir = os.path.join('.', '/content/sample_data/data')\n","# print(f\"My current working directory is: {csv_dir} \")"],"metadata":{"id":"-JZvUPKdVBS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Next step to print Vector size for evey dataset\n","#### Uncomment the code below and run it"],"metadata":{"id":"_KDlNW0iVNe8"}},{"cell_type":"code","source":["\n","# all_files = sorted(glob(os.path.join(csv_dir, 'HTCP*.csv')), \n","#                    key=lambda x: os.stat(x).st_size)\n","\n","# all_df_dict = {os.path.splitext(os.path.basename(x))[0]: pd.read_csv(x) for x in all_files}\n","# print(\"VECTOR SIZE FOR EVERY DATASET:\\n\")\n","# for c_key in all_df_dict.keys():\n","#     print(c_key, 'vector length:',  \n","#           all_df_dict[c_key].shape[1], '->', \n","#             know_image_dim(all_df_dict[c_key].shape[1]))"],"metadata":{"id":"lYP-GEl5Vdvf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print directory names and filenames as a list []\n","#### Uncomment all_files and execute all_files\n"],"metadata":{"id":"T3BwkRcuRJb9"}},{"cell_type":"code","source":["#all_files"],"metadata":{"id":"oBSsG93uRoGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read csv file from list(all_files)\n","data = pd.read_csv(all_files[2])"],"metadata":{"id":"t5pMU4SYHe9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's create an array with labels (not yet one-encoded) and one for the images."],"metadata":{"id":"xmGxOrgpIA_h"}},{"cell_type":"code","source":["# Get the labels from the data\n","labels = data['label']\n","data = data.drop(['label'], axis = 1)"],"metadata":{"id":"PoO_0hLaza48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at the first records of the dataframe"],"metadata":{"id":"UIq683FVLzFD"}},{"cell_type":"markdown","source":["####  Let's look at the first records of the dataframe\n","##### uncomment data.head() and run"],"metadata":{"id":"zO9U2LxUSIKP"}},{"cell_type":"code","source":["# data.head()"],"metadata":{"id":"1Q2EDQktSfnx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The datasets has 5000 images, each 28x28 in gray"],"metadata":{"id":"KuF4ORp_MBJK"}},{"cell_type":"markdown","source":["# Uncomment data shape after dorping the lable and run"],"metadata":{"id":"OmL6AAJiSrCV"}},{"cell_type":"code","source":["# data.shape"],"metadata":{"id":"6JiZgbYUS312"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Examples of each class"],"metadata":{"id":"lfbe465SMZkx"}},{"cell_type":"markdown","source":["Let's plot an image of each class\n","# Uncomment the code below and run it"],"metadata":{"id":"aoD8ytvDMcxG"}},{"cell_type":"code","source":["# # Unique lables\n","# def unique_lables(lable):\n","#   uniq = lable.unique()\n","#   return f\"{uniq}, dtype=int64\"\n","# unique_lables(labels)"],"metadata":{"id":"82sZJWS6UFaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: A function to get Label names defined above\n","# Get all 8 Labels with random data\n","def get_random_element_with_label (data, lbls, lbl):\n","    tmp = lbls == lbl\n","    subset = data[tmp]\n","    return subset.iloc[randint(1,subset.shape[0])]\n","\n","labels_overview = np.empty([10,784])\n","for i in range (1,9):\n","    img = get_random_element_with_label(data, labels, i)\n","    labels_overview[i,:] = img\n"," "],"metadata":{"id":"Ijy644DmzIkp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get all 8 Labels with random data\n","### Uncomment the code below and run it "],"metadata":{"id":"TYFGjMonUakv"}},{"cell_type":"code","source":["# f = plt.figure(figsize=(8,15));\n","# count = 1\n","# for i in range(1,9):\n","#     plt.subplot(5,2,count)\n","#     count = count + 1\n","#     plt.subplots_adjust(hspace=0.2)\n","#     plt.title(get_label_name(i))\n","#     some_digit_image = labels_overview[i,:].reshape(28,28)\n","#     plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n","#     plt.axis(\"off\")"],"metadata":{"id":"kv5KXRY6Ur7l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Work with the original images"],"metadata":{"id":"V2PDGn2qPzqZ"}},{"cell_type":"markdown","source":["Let's first load the images and their information (metadata)\n","##### Join your directory file with Kather_texture_2016_image_tiles_5000 as a Dataframe\n","1. Uncomment the code below and run it\n","\n"],"metadata":{"id":"WDCcjCrMP4Vi"}},{"cell_type":"code","source":["\n","\n","# base_tile_dir = os.path.join('data', '/content/Kather_texture_2016_image_tiles_5000')\n","# print(base_tile_dir)\n","\n","# tile_df = pd.DataFrame({\n","#     'path': glob(os.path.join(base_tile_dir, '*', '*.tif'))\n","# })\n","\n","# tile_df['file_id'] = tile_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\n","# tile_df['cell_type'] = tile_df['path'].map(lambda x: os.path.basename(os.path.dirname(x))) \n","# tile_df['cell_type_idx'] = tile_df['cell_type'].map(lambda x: int(x.split('_')[0]))\n","# tile_df['cell_type'] = tile_df['cell_type'].map(lambda x: x.split('_')[1])\n","# tile_df['full_image_name'] = tile_df['file_id'].map(lambda x: x.split('_Row')[0])\n","# tile_df['full_image_row'] = tile_df['file_id'].map(lambda x: int(x.split('_')[-3]))\n","# tile_df['full_image_col'] = tile_df['file_id'].map(lambda x: int(x.split('_')[-1]))\n","# tile_df.sample(3)"],"metadata":{"id":"BdRv__gnXbqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get brief summary of the dataset\n",". Uncomment the code below and run"],"metadata":{"id":"6Ji2-vbmYLp0"}},{"cell_type":"code","source":["#tile_df.describe(exclude=[np.number])"],"metadata":{"id":"f89rMvRTYf6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# visualize your data\n",". Uncomment the code below and run it"],"metadata":{"id":"HyyqVUt8Ykk4"}},{"cell_type":"code","source":["# fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\n","# c = ['red', 'teal', 'brown', 'olive', 'orange', 'purple', 'gold','gray']\n","# tile_df['cell_type'].value_counts().plot(kind='bar', color= c, ax=ax1)\n"],"metadata":{"id":"bz5pKopOY35Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load in all of the images\n",". uncomment the code below and run it"],"metadata":{"id":"LHW2NkJKZUvy"}},{"cell_type":"code","source":["\n","# from skimage.io import imread\n","# tile_df['image'] = tile_df['path'].map(imread)"],"metadata":{"id":"tSRfZq8Lxh7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tile_df['path'].iloc[0]"],"metadata":{"id":"DGiQO0PoZgba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Show a few in each category"],"metadata":{"id":"O6_kXb4YR9re"}},{"cell_type":"markdown","source":["__NOTE__: the images are here 150x150, so slightly bigger than the gray level ones, that are 28x28.\n","\n",". Uncomment code below and run it"],"metadata":{"id":"kb-L35wMSDxF"}},{"cell_type":"code","source":["n_samples = 5\n","fig, m_axs = plt.subplots(8, n_samples, figsize = (4*n_samples, 3*8))\n","for n_axs, (type_name, type_rows) in zip(m_axs, \n","                                         tile_df.sort_values(['cell_type']).groupby('cell_type')):\n","    n_axs[0].set_title(type_name)\n","    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=2018).iterrows()):\n","        c_ax.imshow(c_row['image'])\n","        c_ax.axis('off')"],"metadata":{"id":"jjw8bUUqZ9WC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load the 5000x5000 images"],"metadata":{"id":"Ceo6B4eDSN-p"}},{"cell_type":"markdown","source":["__NOTE__: keep in mind that each image is roughly 70 Mb, so if you want to try to identify the location of each type of tissue you can try loading one at a time."],"metadata":{"id":"q9mTiVJ-SW5l"}},{"cell_type":"code","source":["base_tile_dir = os.path.join('data', '/content/Kather_texture_2016_larger_images_10')\n","\n","big_image_df = pd.DataFrame({\n","    'path': glob(os.path.join(base_tile_dir,  '*.tif'))\n","})\n","\n","print(big_image_df.iloc[0])"],"metadata":{"id":"t76WpRtkSSmy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cb722c3-0bc4-4724-e1af-5988bcb9d2bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["path    /content/Kather_texture_2016_larger_images_10/...\n","Name: 0, dtype: object\n"]}]},{"cell_type":"code","source":["big_image_df['image'] = big_image_df['path'].map(plt.imread)"],"metadata":{"id":"k3Ut6gygSieQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Now, print The Paths and images\n",". Uncomment code and run it"],"metadata":{"id":"WDGswsg7afIO"}},{"cell_type":"code","source":["# big_image_df.head()"],"metadata":{"id":"e6KhlwhWao45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# the ```big_image_df``` has 10 images.\n",".Uncomment code below and run it"],"metadata":{"id":"iBjuJr55SrBU"}},{"cell_type":"code","source":["# plt.figure(figsize=(10,10))\n","# plt.imshow(big_image_df['image'].iloc[1])\n","# plt.show()"],"metadata":{"id":"AkMSvhifbDjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline models with 28x28 gray images"],"metadata":{"id":"EUHVnQgNT1-k"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","sample_id_count = list(all_df_dict.values())[0].shape[0]\n","train_ids, test_ids = train_test_split(range(sample_id_count), \n","                                       test_size=0.25, \n","                                       random_state=2018)"],"metadata":{"id":"_KhByS1wTz97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","def evaluate_models(in_model_maker):\n","    fig, m_axs = plt.subplots(1, 5, figsize = (25, 5))\n","    for c_ax, c_key in zip(m_axs, all_df_dict.keys()):\n","        # c_key is for example HTCP_8_8_L (the file/type name)\n","        c_df = all_df_dict[c_key].copy()\n","        c_label = c_df.pop('label') # return column and drop from dataframe\n","        c_model = in_model_maker() # function of the model\n","        c_model.fit(c_df.iloc[train_ids, :], c_label.iloc[train_ids]) # fit of the model\n","        c_pred = c_model.predict(c_df.iloc[test_ids, :]) # prediction\n","        sns.heatmap(confusion_matrix(c_label.iloc[test_ids], c_pred), \n","                    annot=True, cbar=False, fmt='d', ax=c_ax)\n","        c_ax.set_title(f'Accuracy: {accuracy_score(c_label[test_ids],c_pred)*100:2.2f}%\\n{c_key}')"],"metadata":{"id":"xarPvWtfT6eq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now, let's see which of the Classifier will perform better result"],"metadata":{"id":"atAlGlYibNTM"}},{"cell_type":"markdown","source":["# Starting with Neireast neighbor Classifier\n","### __Classifier__: Neireast neighbor\n","#### Uncomment the code below and run it"],"metadata":{"id":"nStoxZ6NUN9k"}},{"cell_type":"code","source":["# from sklearn.neighbors import KNeighborsClassifier\n","# evaluate_models(lambda : KNeighborsClassifier(n_jobs=4))"],"metadata":{"id":"qhq1HoO-cKlD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### __Classifier__: Logistic regression\n","####Uncomment the code below and run it"],"metadata":{"id":"rGBxT38JUlSG"}},{"cell_type":"code","source":["# from sklearn.linear_model import LogisticRegression\n","# evaluate_models(lambda : LogisticRegression(n_jobs=4, solver='lbfgs'))"],"metadata":{"id":"1vxy9qk0dE-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### __Classifier__: Random Forest \n","####Uncomment the code below and run it"],"metadata":{"id":"WTdz6hYjU8iv"}},{"cell_type":"code","source":["# from sklearn.ensemble import RandomForestClassifier\n","# evaluate_models(lambda : RandomForestClassifier(n_jobs=4))"],"metadata":{"id":"ulfIOBmcdUga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Compared to Logistic Regression and K-Nearest Neighbors, Random Forest performance (Accuracy) is the best!"],"metadata":{"id":"mvM839eASOlW"}},{"cell_type":"markdown","source":["# Congratulation ! end of this exercise, We hope you enjoyed! \n","### __You have learned __\n","\n","1. Import the dataset\n","2. Find your working directory path\n","3. Split the data to training set (x) data, and test set labels(y)\n","4. Fit your model (training data)\n","5. Visualize your data\n","6. Which classifier is better accuracy\n","\n"],"metadata":{"id":"ifqeH-KgT3QS"}}]}